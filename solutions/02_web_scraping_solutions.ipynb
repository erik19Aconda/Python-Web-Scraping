{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Beautiful Soup: Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFeatureNotFound\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m src = req.text\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Parse the response into an HTML tree\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m soup = \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlxml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#Se realiza un peticion GET a la pagina \u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#Lee el conetendo html de la pagina \u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Convierte el html de la pagina en objetos para facilitar el tratamiento de datos. \u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\bs4\\__init__.py:364\u001b[39m, in \u001b[36mBeautifulSoup.__init__\u001b[39m\u001b[34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m     possible_builder_class = builder_registry.lookup(*features)\n\u001b[32m    363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m possible_builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[32m    365\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a tree builder with the features you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    366\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m. Do you need to install a parser library?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    367\u001b[39m             % \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(features)\n\u001b[32m    368\u001b[39m         )\n\u001b[32m    369\u001b[39m     builder_class = possible_builder_class\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n",
      "\u001b[31mFeatureNotFound\u001b[39m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "# Make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp')\n",
    "# Read the content of the server’s response\n",
    "src = req.text\n",
    "# Parse the response into an HTML tree\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "#Se realiza un peticion GET a la pagina \n",
    "#Lee el conetendo html de la pagina \n",
    "# Convierte el html de la pagina en objetos para facilitar el tratamiento de datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Find All\n",
    "\n",
    "Use Beautiful Soup to find all the `a` elements with class `mainmenu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\"a.mainmenu\")\n",
    "\n",
    "#Se busca elementos html con el selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Extract Specific Attributes\n",
    "\n",
    "Extract all `href` attributes for each `mainmenu` URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[link['href'] for link in soup.select(\"a.mainmenu\")]\n",
    "\n",
    "#Se extrae todos los atributos de elemento a.mainmenu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Get `href` elements pointing to members' bills\n",
    "\n",
    "The code above retrieves information on:  \n",
    "\n",
    "- the senator's name,\n",
    "- their district number,\n",
    "- and their party.\n",
    "\n",
    "We now want to retrieve the URL for each senator's list of bills. Each URL will follow a specific format. \n",
    "\n",
    "The format for the list of bills for a given senator is:\n",
    "\n",
    "`http://www.ilga.gov/senate/SenatorBills.asp?GA=98&MemberID=[MEMBER_ID]&Primary=True`\n",
    "\n",
    "to get something like:\n",
    "\n",
    "`http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True`\n",
    "\n",
    "in which `MEMBER_ID=1911`. \n",
    "\n",
    "You should be able to see that, unfortunately, `MEMBER_ID` is not currently something pulled out in our scraping code.\n",
    "\n",
    "Your initial task is to modify the code above so that we also **retrieve the full URL which points to the corresponding page of primary-sponsored bills**, for each member, and return it along with their name, district, and party.\n",
    "\n",
    "Tips: \n",
    "\n",
    "* To do this, you will want to get the appropriate anchor element (`<a>`) in each legislator's row of the table. You can again use the `.select()` method on the `row` object in the loop to do this — similar to the command that finds all of the `td.detail` cells in the row. Remember that we only want the link to the legislator's bills, not the committees or the legislator's profile page.\n",
    "* The anchor elements' HTML will look like `<a href=\"/senate/Senator.asp/...\">Bills</a>`. The string in the `href` attribute contains the **relative** link we are after. You can access an attribute of a BeatifulSoup `Tag` object the same way you access a Python dictionary: `anchor['attributeName']`. See the <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/#tag\">documentation</a> for more details.\n",
    "* There are a _lot_ of different ways to use BeautifulSoup to get things done. whatever you need to do to pull the `href` out is fine.\n",
    "\n",
    "The code has been partially filled out for you. Fill it in where it says `#YOUR CODE HERE`. Save the path into an object called `full_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "# Read the content of the server’s response\n",
    "src = req.text\n",
    "# Soup it\n",
    "soup = BeautifulSoup(src, \"lxml\")\n",
    "\n",
    "# Create empty list to store our data\n",
    "#SE CREA UN ARRAY VACIO PARA ALMACENAR LOS DATOS\n",
    "members = []\n",
    "\n",
    "\n",
    "# Returns every ‘tr tr tr’ css selector in the page\n",
    "##SELECCIONAMOS TODAS LAS FILAS DE LA TABLA EN CONTRADA \n",
    "rows = soup.select('tr tr tr')\n",
    "# Get rid of junk rows\n",
    "\n",
    "#FILTRA LAS FILAS QUE TIENEN LA CALSE TD.DETAIL\n",
    "rows = [row for row in rows if row.select('td.detail')]\n",
    "\n",
    "# Loop through all rows\n",
    "##LAZO PARA ITERRAR TODAS LAS FILAS \n",
    "for row in rows:\n",
    "\n",
    "    #OBTIENE LOS DATOS \n",
    "    # Select only those 'td' tags with class 'detail'\n",
    "    detail_cells = row.select('td.detail') \n",
    "    # Keep only the text in each of those cells\n",
    "    row_data = [cell.text for cell in detail_cells]\n",
    "    # Collect information\n",
    "    name = row_data[0]\n",
    "    district = int(row_data[3])\n",
    "    party = row_data[4]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Extract href\n",
    "    href = row.select('a')[1]['href']\n",
    "    # Create full path\n",
    "    full_path = \"http://www.ilga.gov/senate/\" + href + \"&Primary=True\"\n",
    "    \n",
    "    # Store in a tuple\n",
    "    senator = (name, district, party, full_path)\n",
    "    # Append to list\n",
    "    members.append(senator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SE OBTENE LOS PRIMEROS 5 ELEMENTOS DE LA TABLA\n",
    "\n",
    "members[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Modularize Your Code\n",
    "\n",
    "Turn the code above into a function that accepts a URL, scrapes the URL for its senators, and returns a list of tuples containing information about each senator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCION PARA EXTRAER LA INFORMACION DEL SITIO WEB\n",
    " #1. Obtencion del html\n",
    " #2. Filtrado de información mediante la clase html\n",
    " #3. Iteracion de cada fila del css, para obtener la inforamcion de cada Senador\n",
    " #4. Almacenamientos de datos \n",
    "\n",
    "def get_members(url):\n",
    "    # Make a GET request\n",
    "    req = requests.get(url)\n",
    "    # Read the content of the server’s response\n",
    "    src = req.text\n",
    "    # Soup it\n",
    "    soup = BeautifulSoup(src, \"lxml\")\n",
    "    # Create empty list to store our data\n",
    "    members = []\n",
    "\n",
    "    # Returns every ‘tr tr tr’ css selector in the page\n",
    "    rows = soup.select('tr tr tr')\n",
    "    # Get rid of junk rows\n",
    "    rows = [row for row in rows if row.select('td.detail')]\n",
    "\n",
    "    # Loop through all rows\n",
    "    for row in rows:\n",
    "        # Select only those 'td' tags with class 'detail'\n",
    "        detail_cells = row.select('td.detail') \n",
    "        # Keep only the text in each of those cells\n",
    "        row_data = [cell.text for cell in detail_cells]\n",
    "        # Collect information\n",
    "        name = row_data[0]\n",
    "        district = int(row_data[3])\n",
    "        party = row_data[4]\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Extract href\n",
    "        href = row.select('a')[1]['href']\n",
    "        # Create full path\n",
    "        full_path = \"http://www.ilga.gov/senate/\" + href + \"&Primary=True\"\n",
    "\n",
    "        # Store in a tuple\n",
    "        senator = (name, district, party, full_path)\n",
    "        # Append to list\n",
    "        members.append(senator)\n",
    "    return(members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFeatureNotFound\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test your code!\u001b[39;00m\n\u001b[32m      2\u001b[39m url = \u001b[33m'\u001b[39m\u001b[33mhttp://www.ilga.gov/senate/default.asp?GA=98\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m senate_members = \u001b[43mget_members\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mlen\u001b[39m(senate_members)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mget_members\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m      5\u001b[39m src = req.text\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Soup it\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m soup = \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlxml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Create empty list to store our data\u001b[39;00m\n\u001b[32m      9\u001b[39m members = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\bs4\\__init__.py:364\u001b[39m, in \u001b[36mBeautifulSoup.__init__\u001b[39m\u001b[34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m     possible_builder_class = builder_registry.lookup(*features)\n\u001b[32m    363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m possible_builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[32m    365\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a tree builder with the features you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    366\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m. Do you need to install a parser library?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    367\u001b[39m             % \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(features)\n\u001b[32m    368\u001b[39m         )\n\u001b[32m    369\u001b[39m     builder_class = possible_builder_class\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n",
      "\u001b[31mFeatureNotFound\u001b[39m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "# Test your code!\n",
    "\n",
    "#SE LLAMA LA FUNCION GET_MEMBERS, ANTES DEFINIDA Y SE ESTABLE LA DIRECCION LA WEB\n",
    "# VERIFICA LA CANTIDAD DE SENADORES EXTRAIDOS.\n",
    " \n",
    "url = 'http://www.ilga.gov/senate/default.asp?GA=98'\n",
    "senate_members = get_members(url)\n",
    "len(senate_members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-home Challenge: Writing a Scraper Function\n",
    "\n",
    "We want to scrape the webpages corresponding to bills sponsored by each bills.\n",
    "\n",
    "Write a function called `get_bills(url)` to parse a given bills URL. This will involve:\n",
    "\n",
    "  - requesting the URL using the <a href=\"http://docs.python-requests.org/en/latest/\">`requests`</a> library\n",
    "  - using the features of the `BeautifulSoup` library to find all of the `<td>` elements with the class `billlist`\n",
    "  - return a _list_ of tuples, each with:\n",
    "      - description (2nd column)\n",
    "      - chamber (S or H) (3rd column)\n",
    "      - the last action (4th column)\n",
    "      - the last action date (5th column)\n",
    "      \n",
    "This function has been partially completed. Fill in the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_bills(url):\n",
    "    src = requests.get(url).text\n",
    "    soup = BeautifulSoup(src)\n",
    "    rows = soup.select('tr tr tr')\n",
    "    bills = []\n",
    "    # Iterate over rows\n",
    "    for row in rows:\n",
    "        # Grab all bill list cells\n",
    "        cells = row.select('td.billlist')\n",
    "        # Keep in mind the name of the senator is not a billlist class!\n",
    "        if len(cells) == 5:\n",
    "            row_text = [cell.text for cell in cells]\n",
    "            # Extract info from row text\n",
    "            bill_id = row_text[0]\n",
    "            description = row_text[1]\n",
    "            chamber = row_text[2]\n",
    "            last_action = row_text[3]\n",
    "            last_action_date = row_text[4]\n",
    "            # Consolidate bill info\n",
    "            bill = (bill_id, description, chamber, last_action, last_action_date)\n",
    "            bills.append(bill)\n",
    "    return bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'senate_members' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m test_url = \u001b[43msenate_members\u001b[49m[\u001b[32m0\u001b[39m][\u001b[32m3\u001b[39m]\n\u001b[32m      2\u001b[39m get_bills(test_url)[\u001b[32m0\u001b[39m:\u001b[32m5\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'senate_members' is not defined"
     ]
    }
   ],
   "source": [
    "test_url = senate_members[0][3]\n",
    "get_bills(test_url)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape All Bills\n",
    "\n",
    "Finally, create a dictionary `bills_dict` which maps a district number (the key) onto a list of bills (the value) coming from that district. You can do this by looping over all of the senate members in `members_dict` and calling `get_bills()` for each of their associated bill URLs.\n",
    "\n",
    "**NOTE:** please call the function `time.sleep(1)` for each iteration of the loop, so that we don't destroy the state's web site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills_dict = {}\n",
    "for member in senate_members[:5]:\n",
    "    bills_dict[member[1]] = get_bills(member[3])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bills_dict[52])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
